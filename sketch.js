// p5 + Matrix visual + Vosk STT via WebSocket
// Ambient ‚Üí Text now maps to dataset words (sound/digit/color) and transcript wraps nicely.

// ---------- Matrix / audio visual ----------
let mic, fft;
let matrix = [];
let colorData = [];
const matrixWidth = 40, matrixHeight = 40, cellSize = 10;
let isMicActive = false;

// ---------- UI ----------
// unified transcript UI ids
const TRANSCRIPT_BOX_ID = "transcript-box";
const TRANSCRIPT_ID     = "transcript";
const LIVE_ID           = "live-line";

let statusDiv; // keep status bar

let startAudioBtn, stopAudioBtn;
let connectBtn, pttBtn, stopSttBtn, clearBtn, contBtn;
let ledSpan, txStats;

// Ambient encoder UI
let ambStartBtn, ambStopBtn, ambSensSlider, ambRateSlider, ambLabel;


// ---------- STT (WebSocket to Vosk) ----------
let ws = null;
let audioCtx, workletNode, stream;
let capturing = false;
let bytesSent = 0, packets = 0;

// Ambient encoder state
let ambientTimer = null;
let ambientPrev = "";
const DIGIT_WORD = ["zero","one","two","three","four","five","six","seven","eight","nine"];
let tokenCount = 0;               // for soft line breaks
const TOKENS_PER_LINE = 14;       // wrap every N tokens visually
let ambientRepeatTicks = 0;
const AMBIENT_FORCE_EVERY = 10;   // append same phrase every N ticks (N * rate ms)

let PHRASE_TO_COLORS = new Map();
let COLOR_KEY_TO_RGB = new Map();
let COLOR_TO_WORDS = new Map(); 
// RGB key -> array of sound tokens from dataset
const COLOR_KEY_TO_SOUND = new Map();


// ===== Ambient Story/NLG =====
let storyTimer = null;
let storyContext = null;   // persistent characters/setting/mood
const STORY_FORCE_EVERY = 6; // even if emotion stable, write every N ticks

// smoothing of the emotion stream (moving average over last K ticks)
const EMO_SMOOTH = 5;
let emoWindow = []; // [{val,aro,dom,label}]

// ----------- SOUND TOKEN ‚Üí VAD (valence [-1..1], arousal [0..1], dominance [0..1]) -----------
const SOUND_TO_VAD = [
  // calm/positive
  {p:/\b(harmony|song|cheer|positive\s*cadence)\b/,            v:+0.65, a:0.40, d:0.55},
  {p:/\b(pause|dramatic\s*pause|hold|rest)\b/,                 v:+0.10, a:0.15, d:0.40},
  {p:/\b(pathway|plot|route|flow)\b/,                          v:+0.30, a:0.35, d:0.55},
  // upward / energy
  {p:/\b(up\s*tone|rise|climb|lift|uptone)\b/,                 v:+0.25, a:0.60, d:0.65},
  // downward / settle
  {p:/\b(down\s*tone|fall|fade|lower|downtone)\b/,             v:-0.05, a:0.25, d:0.35},
  // rough / harsh
  {p:/\b(rumble|growl|grit|crackle|burst|knot)\b/,             v:-0.35, a:0.75, d:0.65},
  // neutral / structural
  {p:/\b(scale|array|block|mesh)\b/,                           v:+0.00, a:0.35, d:0.55},
  // negative / pull back
  {p:/\b(lose|shrink|cry|drop)\b/,                             v:-0.45, a:0.55, d:0.40},
];

// default if no rule matched
const SOUND_DEFAULT = {v:0.0, a:0.40, d:0.50};


// ----------------------- CIELAB EMOTION ENGINE -----------------------
// cite: Lightness ‚Üë ‚Üí Valence ‚Üë; Chroma ‚Üë (+ red/a*) ‚Üí Arousal & Dominance ‚Üë,
// low L* + high chroma(+red) ‚Üí threat/rage; low L* + low chroma ‚Üí negative-powerless;
// happy = high L* & high chroma; surprised = high L* & yellowish; anger = high a* & low L*;
// disgust = high chroma (red+yellow) & low L*; fear/sad = low all.  (Frontiers Psych 2025)

let emoTimer = null, emoPrev = "", emoRepeatTicks = 0;
const EMO_FORCE_EVERY = 8; // write same phrase every N ticks if unchanged

// ---------- RGB -> Lab ----------
function rgbToXyz(r,g,b){
  r/=255; g/=255; b/=255;
  const lin = (u)=> (u<=0.04045? u/12.92 : Math.pow((u+0.055)/1.055,2.4));
  r=lin(r); g=lin(g); b=lin(b);
  // sRGB D65
  const x = r*0.4124564 + g*0.3575761 + b*0.1804375;
  const y = r*0.2126729 + g*0.7151522 + b*0.0721750;
  const z = r*0.0193339 + g*0.1191920 + b*0.9503041;
  return {x,y,z};
}
function xyzToLab(x,y,z){
  // D65 reference white
  const Xr=0.95047, Yr=1.00000, Zr=1.08883;
  const f=(t)=> (t>0.008856? Math.cbrt(t) : (7.787*t + 16/116));
  const fx=f(x/Xr), fy=f(y/Yr), fz=f(z/Zr);
  const L= (116*fy - 16);
  const a= 500*(fx - fy);
  const b= 200*(fy - fz);
  return {L,a,b};
}
function rgbToLab(r,g,b){ const xyz=rgbToXyz(r,g,b); return xyzToLab(xyz.x,xyz.y,xyz.z); }

// ---------- Lab -> VAD ----------
function labToVAD(L,a,b){
  // Per paper: Valence ~ ‚ÜëL* (+ some chroma), Arousal ~ ‚Üëchroma & ‚Üëred (a*) and ‚ÜìL*,
  // Dominance ~ ‚Üëchroma & ‚Üëred and ‚ÜìL*. Interactions are approximated with weights. :contentReference[oaicite:1]{index=1}
  const C = Math.sqrt(a*a + b*b);            // chroma
  const Ln = constrain(L/100, 0, 1);
  const Cn = constrain(C/100, 0, 1);         // normalize chroma roughly to [0,1]
  const an = constrain((a+100)/200, 0, 1);   // redness [0,1] (centered around 0)

  // weights chosen to reflect effects in the paper
  let val = 0.65*Ln + 0.25*Cn - 0.05*(1-an);         // brighter & more colorful => more positive
  let aro = 0.55*Cn + 0.30*an - 0.25*Ln;             // colorful/red raises arousal, darkness raises too
  let dom = 0.45*Cn + 0.35*an - 0.20*Ln;             // colorful/red dominant, darkness dominant

  // interaction: if (an high AND Cn high) boost arousal & dominance (‚Äúrage/joy effect‚Äù)
  const boost = Math.max(0, (an-0.55)) * Math.max(0, (Cn-0.55));
  aro += 0.15*boost; dom += 0.15*boost;

  // clip
  val = constrain(val,0,1); aro = constrain(aro,0,1); dom = constrain(dom,0,1);
  return {val, aro, dom, C};
}

// ---------- Lab -> discrete emotion ----------
function labToEmotion(L,a,b){
  const {val, aro, dom, C} = labToVAD(L,a,b);
  const light = L/100, red = (a>0)? a/80 : 0, yell = (b>0)? b/80 : 0, chrom = C/100;

  // rule set distilled from paper results/tables (cf. Figs 2‚Äì7 & Tables) :contentReference[oaicite:2]{index=2}
  if (light>0.70 && chrom>0.45 && red<0.35) return "happy";
  if (light>0.65 && yell>0.35) return "surprised";
  if (red>0.55 && light<0.55 && chrom>0.40) return "anger";
  if (light<0.55 && chrom>0.45 && (red>0.35 || yell>0.35)) return "disgust";
  if (light<0.45 && chrom<0.35) return (val<0.45 ? "sad" : "fear");        // both low; fear slightly lighter/darker split
  // fallback by VAD quadrants
  if (val>0.65 && aro<0.45) return "calm";
  if (val>0.65 && aro>=0.45) return "elation";
  if (val<0.40 && aro>=0.55) return "threat";
  return (val<0.45? "boredom" : "content");
}

// ---------- scan recent rows -> top emotions ----------
function ambientEmotionsFromMatrixLab(nRows=4){
  const n = Math.min(nRows, matrix.length);
  if (n===0) return [];
  const tally = new Map();
  for (let r = matrix.length - n; r < matrix.length; r++){
    const row = matrix[r];
    for (let c = 0; c < row.length; c++){
      const cell = row[c];
      const LAb = rgbToLab(cell.r, cell.g, cell.b);
      const emo = labToEmotion(LAb.L, LAb.a, LAb.b);
      tally.set(emo, (tally.get(emo)||0)+1);
    }
  }
  const K = Math.max(1, Math.floor(map(ambSensSlider.value()/100, 0,1, 1,4)));
  return Array.from(tally.entries()).sort((a,b)=>b[1]-a[1]).slice(0,K).map(x=>x[0]);
}

// ---------- controller ----------
function startEmotionsVAD(){
  if (!isMicActive){ msg("‚ùó Click üéß Start Audio first."); return; }
  stopEmotionsVAD();
  const rate = ambRateSlider.value();
  emoPrev = ""; emoRepeatTicks = 0; tokenCount = 0;
  emoTimer = setInterval(()=>{
    const words = ambientEmotionsFromMatrixLab(4);
    if (!words.length) return;
    const phrase = words.join(" ");
    setLive("<b>Listening:</b> " + phrase);

    if (phrase !== emoPrev){
      emoPrev = phrase; emoRepeatTicks = 0;
      appendFinal(phrase + " ");
      return;
    }
    emoRepeatTicks++;
    if (emoRepeatTicks >= EMO_FORCE_EVERY){
      emoRepeatTicks = 0;
      appendFinal(phrase + " ");
    }
  }, rate);
  msg(`üß≠ Emotions (CIELAB) ‚Ä¶ ${rate}ms/sample.`);
}
function stopEmotionsVAD(){
  if (emoTimer){ clearInterval(emoTimer); emoTimer = null; }
  msg("‚õî Emotions stopped.");
}


// -------------------- LOCAL DATASET LOADER --------------------
const DATASET_FILE = "semantic_rgb_mapping_with_sentiment.csv"; 
let datasetTable = null;

function preload() {
  // Synchronous in p5: when preload() returns, the table is ready.
  datasetTable = loadTable(DATASET_FILE, "csv", "header");
}

function setup(){
  let cnv = createCanvas(matrixWidth * cellSize, matrixHeight * cellSize);
  cnv.parent("canvas-container"); 
  mic = new p5.AudioIn();
  fft = new p5.FFT(0.8, 1024);
  fft.setInput(mic);
  buildDatasetFromTable(datasetTable);   // <<< make data usable everywhere
  createUI();
  textFont("system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial");
  textSize(12);

  // Resume audio on first interaction (works on GitHub Pages, iOS, etc.)
  const boot = () => {
    userStartAudio().then(() => {
      console.log('üîä audio context resumed');
    }).catch(e => console.warn('audio resume failed', e));
    window.removeEventListener('pointerdown', boot, {passive:true});
  };
  window.addEventListener('pointerdown', boot, {passive:true});
}

function soundTokenToVAD(tok){
  const t = (tok||"").toLowerCase().trim();
  for (const rule of SOUND_TO_VAD){
    if (rule.p.test(t)) return {v:rule.v, a:rule.a, d:rule.d};
  }
  return SOUND_DEFAULT;
}

// Map discrete emotion -> sentiment scores (valence [-1..1], arousal [0..1], dominance [0..1])
const EMO_PROFILE = {
  calm:   {v: 0.6, a:0.2, d:0.4},
  content:{v: 0.7, a:0.3, d:0.5},
  joy:    {v: 0.9, a:0.6, d:0.6},
  elation:{v: 1.0, a:0.8, d:0.7},
  trust:  {v: 0.7, a:0.4, d:0.6},
  anticipation:{v:0.6,a:0.6,d:0.5},
  surprise:{v:0.5,a:0.8,d:0.4},
  anger:  {v:-0.7,a:0.8,d:0.7},
  disgust:{v:-0.8,a:0.7,d:0.6},
  fear:   {v:-0.8,a:0.8,d:0.3},
  sadness:{v:-0.9,a:0.3,d:0.2},
  boredom:{v:-0.3,a:0.1,d:0.3},
  threat: {v:-0.6,a:0.7,d:0.5},
  happy:  {v:0.9,a:0.6,d:0.6}
};

// Templates per emotion (short, ambient-friendly)
const TEMPLATES = {
  calm: [
    "the room loosens its shoulders",
    "air settles into a soft rhythm",
    "quiet turns warm around the edges"
  ],
  content: [
    "things feel gently right where they are",
    "small comforts gather like folded light",
    "the present tucks itself in neatly"
  ],
  joy: [
    "color lifts and the heart says yes",
    "a sudden brightness finds a grin",
    "steps feel lighter without asking"
  ],
  elation: [
    "everything wants to laugh at once",
    "bright rushes bloom under the skin",
    "the day sparks, leaping from tile to tile"
  ],
  trust: [
    "hands rest open on the table",
    "the floor holds; the walls keep their word",
    "breath arrives on time"
  ],
  anticipation: [
    "a hinge of possibility creaks wider",
    "something almost-started hums",
    "edges ring with a waiting note"
  ],
  surprise: [
    "a bright bead pops in the quiet",
    "the surface ripples‚Äîthen smooths",
    "a quick blink of lightning under glass"
  ],
  anger: [
    "heat climbs the rails of the room",
    "edges sharpen; vowels bite",
    "a red pulse taps the ribs"
  ],
  disgust: [
    "color curdles at the rim",
    "the tongue pulls back from the cup",
    "something sour fogs the air"
  ],
  fear: [
    "shadows practice being doors",
    "the throat rehearses silence",
    "footsteps measure the dark"
  ],
  sadness: [
    "blue gathers its long coat",
    "the hour sits heavy and kind",
    "a low rain thinks in circles"
  ],
  boredom: [
    "time flattens into paper",
    "a thin hum holds the room together",
    "the seconds forget their names"
  ],
  threat: [
    "distance leans in a fraction",
    "the corners keep a watch",
    "metal in the air, not seen"
  ],
  happy: [
    "even the dust chooses to dance",
    "windows smile without glass",
    "every ordinary thing answers back"
  ]
};

// Lightweight connectors based on trajectories
const CONNECTORS = {
  rise:  ["and then", "so", "next", "building,", "after that"],
  fall:  ["until", "and slowly", "at last", "finally", "then"],
  turn:  ["but", "however", "yet", "still", "meanwhile"],
  flat:  [",", "‚Äî", "‚Ä¶", "and", " "]
};

function startStory(){
  if (!isMicActive){ msg("‚ùó Click üéß Start Audio first."); return; }
  stopStory();

  // seed context once per run
  storyContext = {
    who: choice(["we","the room","the city","the hallway","the street","the window"]),
    place: choice(["kitchen","river edge","studio","stairs","waiting room","train"]),
    lastVal: 0, lastAro: 0, lastEmo: ""
  };
  emoWindow = [];
  tokenCount = 0;

  const rate = ambRateSlider.value();
  storyTimer = setInterval(()=> {
    const tick = emotionTick(); // {label, val, aro, dom}
    if (!tick) return;

    // smooth window
    emoWindow.push(tick);
    if (emoWindow.length > EMO_SMOOTH) emoWindow.shift();
    const avg = averageEmo(emoWindow);

    const sentence = sentimentToSentence(avg, storyContext);
    if (sentence) appendFinal(sentence + " ");

    // update context
    storyContext.lastVal = avg.val;
    storyContext.lastAro = avg.aro;
    storyContext.lastEmo = avg.label;
    setLive("<b>Listening:</b> " + avg.label);
  }, rate);

  msg(`üìñ Story mode‚Ä¶ ${rate}ms/sample.`);
}

function stopStory(){
  if (storyTimer){ clearInterval(storyTimer); storyTimer = null; }
  msg("‚õî Story mode stopped.");
}

function emotionTick(){
  const n = Math.min(4, matrix.length);
  if (n===0) return null;

  // --- Lab-based average as before ---
  let sumV=0,sumA=0,sumD=0, N=0;
  const freq = new Map();
  for (let r = matrix.length - n; r < matrix.length; r++){
    const row = matrix[r];
    for (let c = 0; c < row.length; c+=2){
      const cell = row[c];
      const {L,a,b} = rgbToLab(cell.r, cell.g, cell.b);
      const vad = labToVAD(L,a,b);
      const emo = labToEmotion(L,a,b);
      sumV += vad.val; sumA += vad.aro; sumD += vad.dom; N++;
      freq.set(emo, (freq.get(emo)||0)+1);
    }
  }
  const labAvg = {val: sumV/N, aro: sumA/N, dom: sumD/N};

  // --- Noise VAD from dataset "sound" column + motion ---
  const noise = noiseVADFromMatrix(n);

  // --- Blend: alpha depends on noise strength & arousal contrast ---
  const alphaBase = 0.35;                 // baseline influence of sound
  const alpha = clamp01(alphaBase + 0.40*noise.strength); // up to ~0.75
  const reg = mixVAD(labAvg, noise, alpha);
}


  const label = Array.from(count.entries()).sort((a,b)=>b[1]-a[1])[0][0];
  return { label, val: clamp01(sumV/N), aro: clamp01(sumA/N), dom: clamp01(sumD/N) };
}

function averageEmo(arr){
  const n = arr.length || 1;
  let v=0,a=0,d=0; const freq=new Map();
  for (const e of arr){ v+=e.val; a+=e.aro; d+=e.dom; freq.set(e.label,(freq.get(e.label)||0)+1); }
  const label = Array.from(freq.entries()).sort((x,y)=>y[1]-x[1])[0][0];
  return { label, val: v/n, aro: a/n, dom: d/n };
}
function clamp01(x){ return Math.max(0, Math.min(1, x)); }

function sentimentToSentence(s, ctx){
  const emo = s.label in TEMPLATES ? s.label : fallbackByVAD(s);
  const pool = TEMPLATES[emo] || ["something moves and means it"];
  let line = choice(pool);

  // add soft context + connectors based on trajectory
  const dv = s.val - ctx.lastVal;
  const da = s.aro - ctx.lastAro;
  const mag = Math.sqrt(dv*dv + da*da);

  let conn;
  if (mag < 0.05) conn = choice(CONNECTORS.flat);
  else if (dv > 0 && da >= 0) conn = choice(CONNECTORS.rise);
  else if (dv < 0 && da <= 0) conn = choice(CONNECTORS.fall);
  else conn = choice(CONNECTORS.turn);

  // sprinkle a subject/location sometimes for cohesion
  if (Math.random() < 0.25) {
    line = `${ctx.who} in the ${ctx.place} ${conn} ${line}`;
  } else {
    line = `${conn} ${line}`;
  }

  // intensity adornments
  if (s.aro > 0.75 && Math.random()<0.3) line += ", quick as a match";
  if (s.val > 0.8 && Math.random()<0.3) line += ", bright and kind";
  if (s.val < 0.35 && Math.random()<0.3) line += ", low and honest";

  // punctuation/pacing
  line = tidySentence(line);
  return line;

  const motifs = ambientSoundsFromMatrix(2); 
  if (motifs.length && Math.random()<0.30){
    line += ` (${motifs.join(" ¬∑ ")})`;
  }
  return tidySentence(line);
}

function fallbackByVAD(s){
  if (s.val>0.7 && s.aro<0.4) return "calm";
  if (s.val>0.7 && s.aro>=0.4) return "joy";
  if (s.val<0.4 && s.aro>=0.6) return "fear";
  if (s.val<0.45 && s.aro<0.4) return "sadness";
  return "content";
}

function tidySentence(t){
  // clean connector leading commas
  t = t.replace(/^[,‚Äì‚Äî\s]+/,"").trim();
  // capitalize first letter, ensure period
  t = t.charAt(0).toUpperCase() + t.slice(1);
  if (!/[.!?]$/.test(t)) t += ".";
  return t;
}


// Rebuild global data structures from a p5.Table
function buildDatasetFromTable(table) {
  colorData = [];
  PHRASE_TO_COLORS = new Map();
  COLOR_KEY_TO_RGB = new Map();
  COLOR_TO_WORDS = new Map();

  if (!table || !table.getRowCount()) {
    console.warn("Dataset missing or empty:", DATASET_FILE);
    return;
  }

  const getCol = (row, names) => {
    for (const name of names) {
      const v = row.get(name);
      if (v !== undefined && v !== null && String(v).trim() !== "") return String(v);
    }
    return "";
  };

  for (const row of table.rows) {
    const r = parseInt(getCol(row, ["r","R","red"])) || 0;
    const g = parseInt(getCol(row, ["g","G","green"])) || 0;
    const b = parseInt(getCol(row, ["b","B","blue"])) || 0;

const rawSound = (row.get('sound') || "").toLowerCase().trim();
// split by comma/semicolon OR keep multi-word phrases intact if separated with slashes
const sounds = rawSound
  .split(/[,;]\s*/).map(s => s.trim()).filter(Boolean); // ["dramatic pause","rumble","up tone",...]

COLOR_KEY_TO_SOUND.set(`${r},${g},${b}`, sounds);

    const sound = getCol(row, ["sound","phrase","label","term","semantic","meaning","word","descriptor"]).toLowerCase().trim();
    const colorName = getCol(row, ["Closest Color"]).trim();
    const digit = getCol(row, ["digit","number"]).trim();

    // --- NEW: Original Words list ---
    const origRaw = getCol(row, ["Original Words","tokens"]);
    const origWords = splitWords(origRaw); // -> ["word1","word2",...]

    const entry = { r, g, b, color: colorName, digit, sound };
    colorData.push(entry);

    const rgbKey = `${r},${g},${b}`;
    COLOR_KEY_TO_RGB.set(rgbKey, [r, g, b]);

    // Build phrase -> colors (kept for analyzer)
    if (sound) {
      if (!PHRASE_TO_COLORS.has(sound)) PHRASE_TO_COLORS.set(sound, []);
      PHRASE_TO_COLORS.get(sound).push({ r, g, b, colorName: colorName || sound });
    }

    // Build color -> words (primary mapping we‚Äôll use)
    const colorKey = colorName ? colorName.toLowerCase() : rgbKey;
    if (!COLOR_TO_WORDS.has(colorKey)) COLOR_TO_WORDS.set(colorKey, []);
    if (origWords.length) {
      // merge while avoiding dupes
      const arr = COLOR_TO_WORDS.get(colorKey);
      for (const w of origWords) if (w && !arr.includes(w)) arr.push(w);
    }
  }

  console.log("‚úÖ dataset loaded:", colorData.length, "rows with color‚Üíwords keys:", COLOR_TO_WORDS.size);
}

// split a cell like "word1, word2; word3 | word4" into clean tokens
function splitWords(s) {
  if (!s) return [];
  return s
    .split(/[,;|\/]+/g)
    .map(t => t.toLowerCase().replace(/[_\-]+/g," ").replace(/\s+/g," ").trim())
    .filter(Boolean);
}


function draw() {
  background(0);
  if (isMicActive) {
    const spec = fft.analyze();
    const row = spec.slice(0, matrixWidth).map((freq) => {
      const idx = floor(map(freq, 0, 255, 0, max(0, colorData.length - 1)));
      const d = colorData[idx] || { r:0,g:0,b:0, digit:"", color:"", sound:"" };
      return {
          cellColor: color(d.r, d.g, d.b),
          r: d.r, g: d.g, b: d.b,           
          digit: d.digit,
          colorName: d.color,
          sound: d.sound
      };
    });
    matrix.push(row);
    if (matrix.length > matrixHeight) matrix.shift();
  }
  // render
  for (let y=0; y<matrix.length; y++) {
    for (let x=0; x<matrix[y].length; x++) {
      fill(matrix[y][x].cellColor);
      rect(x*cellSize, y*cellSize, cellSize, cellSize);
      fill(255);
      textAlign(CENTER, CENTER);
      textSize(8);
      text(matrix[y][x].digit, x*cellSize+cellSize/2, y*cellSize+cellSize/2);
    }
  }
}

// ------------------------- UI -------------------------
function createUI(){
  const c = createDiv().style("margin","10px 0"); // or your existing container

  statusDiv = createDiv("Ready. ‚Ä¢ For STT: üéß Start Audio ‚Üí üîå Connect STT ‚Üí talk. ‚Ä¢ For Ambient/Story: üéß Start Audio ‚Üí Start ‚Ä¶")
    .parent(c).addClass("status").style("margin","6px 0");

ledSpan = createSpan("üî¥").parent(c).style("margin-right","6px");
  txStats = createSpan(" packets: 0 ‚Ä¢ bytes: 0 ").parent(c);
  createElement("br").parent(c);

  startAudioBtn = btn("üéß Start Audio", () => {
    mic.start(() => { fft.setInput(mic); isMicActive = true; msg("üéß Mic active."); }, micErr);
  }, c);
  stopAudioBtn = btn("‚èπ Stop Audio", () => {
    try { mic.stop(); } catch(_) {}
    isMicActive = false; msg("üõë Mic stopped.");
  }, c);

  connectBtn = btn("üîå Connect STT", connectSTT, c);
  contBtn    = btn("‚ñ∂Ô∏è Start Continuous", startContinuous, c);
  pttBtn     = btn("üéô Hold to Talk", null, c);
  pttBtn.mousePressed(startPTT);
  pttBtn.mouseReleased(stopPTT);
  stopSttBtn = btn("‚ñ† Stop STT", stopSTT, c);
  clearBtn   = btn("Clear", () => { setLive(""); setFinal(""); tokenCount=0; }, c);

  createElement("hr").parent(c);

  // Ambient encoder controls
  ambLabel = createSpan("üåà Ambient ‚Üí Dataset Words ").parent(c).style("font-weight","600");
  ambStartBtn = btn("Start Ambient", startAmbient, c);
  ambStopBtn  = btn("Stop Ambient", stopAmbient, c);
  btn("Start Emotions (Lab)", startEmotionsVAD, c);
  btn("Stop Emotions", stopEmotionsVAD, c);
  btn("Start Story", startStory, c);
  btn("Stop Story", stopStory, c);


  createSpan("&nbsp; Sensitivity ").parent(c);
  ambSensSlider = createSlider(0, 100, 45, 1).parent(c).style("width","160px");

  createSpan("&nbsp; Rate(ms) ").parent(c);
  ambRateSlider = createSlider(60, 400, 160, 10).parent(c).style("width","160px");

  createElement("br").parent(c);

  // üëá make/attach the unified transcript panel
  ensureTranscriptUI(c);
}

// make transcript panel if missing (works on p5 editor + GitHub Pages)
function ensureTranscriptUI(parentEl){
  if (!select("#"+TRANSCRIPT_BOX_ID)) {
    const box = createDiv().id(TRANSCRIPT_BOX_ID).addClass("panel").parent(parentEl)
      .style("margin","12px 0").style("max-width","var(--maxw)").style("max-height","320px")
      .style("overflow-y","auto");
    createElement("div").id(LIVE_ID).addClass("listening").parent(box)
      .html("<b>Listening:</b> (‚Ä¶)")
      .style("margin","0 0 8px 0");
    createElement("div").id(TRANSCRIPT_ID).addClass("transcript").parent(box)
      .html("<b>Transcript:</b> ");
  }
}

  // WRAPPED transcript box
function appendFinal(text) {
  let transcriptDiv = select('#transcript');
  transcriptDiv.html(transcriptDiv.html() + text + "<br/>");
}

  appendFinal("<span class='live'>" + phrase + "</span>");

function btn(label, handler, parent) {
  const b = createButton(label).parent(parent);
  b.style("padding","8px 12px").style("border-radius","8px").style("border","1px solid #999")
   .style("background","#f5f5f5").style("cursor","pointer").style("margin","6px 8px 6px 0");
  if (handler) b.mousePressed(handler);
  return b;
}

// ------------------------- STT client (Vosk WS) -------------------------
async function connectSTT() {
  if (ws && ws.readyState === WebSocket.OPEN) { msg("WS already connected."); return; }
  ws = new WebSocket("ws://localhost:3001");
  ws.binaryType = "arraybuffer";

  ws.onopen = async () => {
    msg("üì° STT connected. (server ready)");
    led("üü¢");
    bytesSent = 0; packets = 0; showTx();
    await setupAudioWorklet();
  };
  ws.onmessage = (ev) => {
    try {
      const data = JSON.parse(ev.data);
      if (data.type === "partial") setLive(data.value.partial || "");
      if (data.type === "final") {
        const text = (typeof data.value === "string") ? data.value : (data.value.text || "");
        if (text) appendFinal(text + " ");
      }
      if (data.type === "status") msg("üîä " + data.value);
    } catch(e) {
      console.warn("Bad WS message:", e);
    }
  };
  ws.onclose = () => { msg("üîå STT disconnected."); led("üî¥"); };
  ws.onerror = (e) => { console.error(e); msg("‚ùó STT error (WS)."); led("üü†"); };
}

async function setupAudioWorklet() {
  if (audioCtx) return;
  stream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1 }, video: false });
  audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });
  await audioCtx.audioWorklet.addModule("recorder-worklet.js");
  const src = audioCtx.createMediaStreamSource(stream);
  workletNode = new AudioWorkletNode(audioCtx, "downsample-processor");
  workletNode.port.onmessage = (event) => {
    if (event.data && event.data.__init) { console.log("Worklet init:", event.data.__init); return; }
    if (!capturing || !ws || ws.readyState !== WebSocket.OPEN) return;
    const buf = event.data.buffer; // ArrayBuffer from Int16Array
    ws.send(buf);
    bytesSent += buf.byteLength;
    packets++;
    showTx();
  };
  src.connect(workletNode);
}

async function startPTT() {
  if (!ws || ws.readyState !== WebSocket.OPEN) { msg("Connect STT first."); return; }
  if (!audioCtx) await setupAudioWorklet();
  await audioCtx.resume();
  capturing = true;
  ws.send(JSON.stringify({ type: "start" }));
  msg("üü¢ PTT listening‚Ä¶ (hold)");
}
function stopPTT() {
  if (!ws || ws.readyState !== WebSocket.OPEN) return;
  capturing = false;
  ws.send(JSON.stringify({ type: "stop" }));
  msg("üõë PTT stopped.");
}

async function startContinuous() {
  if (!ws || ws.readyState !== WebSocket.OPEN) { msg("Connect STT first."); return; }
  if (!audioCtx) await setupAudioWorklet();
  await audioCtx.resume();
  if (capturing) return;
  capturing = true;
  ws.send(JSON.stringify({ type: "start" }));
  msg("üü¢ Continuous listening‚Ä¶ click ‚ñ† Stop STT to end.");
}

function stopSTT() {
  capturing = false;
  try { ws && ws.close(); } catch(_) {}
  ws = null;
  msg("‚ñ† STT stopped."); led("üî¥");
}

// ------------------------- Ambient ‚Üí DATASET WORDS -------------------------

function startAmbient() {
  if (!isMicActive) { msg("‚ùó Click üéß Start Audio first."); return; }
  if (colorData.length === 0) { msg("‚ùó Dataset not loaded."); return; }

  stopAmbient();
  const rate = ambRateSlider.value();
  ambientPrev = "";
  ambientRepeatTicks = 0;
  tokenCount = 0;

  ambientTimer = setInterval(() => {
    // build words from the most recent rows
    const words = ambientWordsFromMatrix(4); // last 4 rows
    if (!words.length) return;

    const phrase = words.join(" ");
    setLive("<b>Listening:</b> " + phrase);

    // Always append if phrase changed
    if (phrase !== ambientPrev) {
      ambientPrev = phrase;
      ambientRepeatTicks = 0;
      appendFinal(phrase + " ");
      return;
    }

    // If phrase hasn't changed, append it every N ticks so you get a baseline stream
    ambientRepeatTicks++;
    if (ambientRepeatTicks >= AMBIENT_FORCE_EVERY) {
      ambientRepeatTicks = 0;
      appendFinal(phrase + " ");
    }
  }, rate);

  msg(`üåà Ambient encoding‚Ä¶ ${rate}ms/sample. (force every ${AMBIENT_FORCE_EVERY} ticks)`);
}

function stopAmbient() {
  if (ambientTimer) { clearInterval(ambientTimer); ambientTimer = null; }
  msg("‚õî Ambient encoding stopped.");
}



// Look at the last N rows, find the top colors, emit their mapped words
function ambientWordsFromMatrix(nRows=4) {
  const n = min(nRows, matrix.length);
  if (n === 0) return [];

  // Build a dominance histogram keyed by colorName or RGB
  const hist = new Map();
  const sens = ambSensSlider.value()/100;

  for (let r = matrix.length - n; r < matrix.length; r++) {
    const row = matrix[r];
    for (let c = 0; c < row.length; c++) {
      const cell = row[c];
      const key = (cell.colorName && cell.colorName.trim())
        ? cell.colorName.toLowerCase()
        : `${cell.r},${cell.g},${cell.b}`;

      if (!hist.has(key)) {
        hist.set(key, {
          count: 0,
          colorKey: key,
          colorName: cell.colorName,
          r: cell.r, g: cell.g, b: cell.b,
          sound: cell.sound,
          digit: cell.digit
        });
      }
      hist.get(key).count += 1;
    }
  }

  const items = Array.from(hist.values()).sort((a,b)=> b.count - a.count);

  // Choose how many colors to speak per tick based on sensitivity (1..4)
  const K = max(1, floor(map(sens, 0, 1, 1, 4)));
  const chosen = items.slice(0, K);

  // For each chosen color, pick a random word from "Original Words"
  const words = chosen.map(entry => pickWordForColor(entry)).filter(Boolean);

  // Tiny shuffle on ties to avoid stutter patterns
  if (words.length > 1 && items.length > 1 && items[0].count === items[1].count) words.reverse();

  return words;
}


function pickWordForColor(entry) {
  const colorKey = (entry.colorName && entry.colorName.trim())
    ? entry.colorName.toLowerCase()
    : `${entry.r},${entry.g},${entry.b}`;

  // try by color name (preferred)
  let list = COLOR_TO_WORDS.get(colorKey);

  // if name didn‚Äôt hit, try exact RGB
  if ((!list || list.length === 0) && entry.colorName) {
    list = COLOR_TO_WORDS.get(`${entry.r},${entry.g},${entry.b}`);
  }

  if (list && list.length) {
    return choice(list);
  }

  // graceful fallbacks if that color had no words in the dataset
  if (entry.sound && entry.sound.trim()) return tidy(entry.sound);
  const d = parseInt(entry.digit);
  if (!isNaN(d) && d >= 0 && d <= 9) return DIGIT_WORD[d];
  if (entry.colorName && entry.colorName.trim()) return tidy(entry.colorName);

  // last resort: speak the RGB (you‚Äôll still see activity)
  return `${entry.r},${entry.g},${entry.b}`;
}

// utils
function choice(arr){ return arr[Math.floor(Math.random() * arr.length)]; }
function tidy(s){ return s.toLowerCase().replace(/[_\-]+/g," ").replace(/\s+/g," ").trim(); }


function mapColorEntryToWord(entry) {
  // Prefer explicit dataset "sound" field if present
  if (entry.sound && entry.sound.trim()) return tidy(entry.sound);

  // Fallback to digit word
  const d = parseInt(entry.digit);
  if (!isNaN(d) && d >= 0 && d <= 9) return DIGIT_WORD[d];

  // Last resort: color name itself
  if (entry.color && entry.color.trim()) return tidy(entry.color);

  return null;
}
function tidy(s) {
  // Make it pleasant as a token
  return s.toLowerCase().replace(/[_\-]+/g," ").replace(/\s+/g," ").trim();
}

function noiseVADFromMatrix(nRows=4){
  const n = Math.min(nRows, matrix.length);
  if (n===0) return {v:0.0, a:0.0, d:0.0, strength:0};

  let Sv=0, Sa=0, Sd=0, N=0, motionSum=0, motionN=0;

  const start = matrix.length - n;
  for (let r = start; r < matrix.length; r++){
    const row = matrix[r];
    for (let c = 0; c < row.length; c+=2){ // stride for speed
      const cell = row[c];
      const key = `${cell.r},${cell.g},${cell.b}`;
      const tokens = COLOR_KEY_TO_SOUND.get(key);
      if (tokens && tokens.length){
        // take up to 2 tokens per color to avoid overbias
        const take = Math.min(tokens.length, 2);
        for (let i=0;i<take;i++){
          const tok = tokens[(i===0) ? Math.floor(Math.random()*tokens.length) : (i%tokens.length)];
          const s = soundTokenToVAD(tok);
          Sv += s.v; Sa += s.a; Sd += s.d; N++;
        }
      }
      // temporal "burstiness"
      if (r>start){
        const prev = matrix[r-1][c];
        if (prev){
          motionSum += Math.abs(cell.r-prev.r)+Math.abs(cell.g-prev.g)+Math.abs(cell.b-prev.b);
          motionN++;
        }
      }
    }
  }
  if (N===0) return {v:0.0, a:0.0, d:0.0, strength:0};

  let v = Sv/N, a = Sa/N, d = Sd/N;

  // normalize motion 0..765 -> 0..1 and add gentle arousal boost
  const motion = motionN? Math.min(1, motionSum/(motionN*300)) : 0; // 300 is a soft scale
  a = Math.min(1, a + 0.25*motion);

  // strength of the noise signal = how many sound hits we saw (0..1)
  const strength = Math.min(1, N / (n * matrixWidth / 2)); // /2 due to stride

  return {v, a, d, strength};
}
function clamp01(x){ return Math.max(0, Math.min(1, x)); }

// mix two VADs with weight alpha, then clamp
function mixVAD(lab, noise, alpha){
  const v = clamp01((1-alpha)*lab.val + alpha*(noise.v*0.5 + 0.5)); // map [-1..1] -> [0..1]
  const a = clamp01((1-alpha)*lab.aro + alpha*noise.a);
  const d = clamp01((1-alpha)*lab.dom + alpha*noise.d);
  return {val:v, aro:a, dom:d};
}



// ------------------------- UI helpers -------------------------
function micErr(e){ console.error(e); msg("‚ùó Mic permission failed."); }
function msg(t){ statusDiv.html(t); }
function led(icon){ ledSpan.html(icon); }
function showTx(){ txStats.html(` packets: ${packets} ‚Ä¢ bytes: ${bytesSent}`); }
// write a short live line (no scrolling)
function setLive(html){
  const el = select("#"+LIVE_ID);
  if (el) el.html(html);
}

// append to transcript (with optional paragraph break)
function appendFinal(text, opts = { paragraph:false }){
  const el = select("#"+TRANSCRIPT_ID);
  if (!el) return;
  let html = el.html();
  // remove initial "<b>Transcript:</b> " once we start
  if (html.trim().endsWith("</b>")) html += " ";
  html += text + (opts.paragraph ? "<br><br>" : " ");
  el.html(html);
  autoScrollTranscript();
}

// clear transcript + live
function clearTranscript(){
  const t = select("#"+TRANSCRIPT_ID);
  if (t) t.html("<b>Transcript:</b> ");
  setLive("<b>Listening:</b> (‚Ä¶)");
}

// keep the box pinned to bottom
function autoScrollTranscript(){
  const box = select("#"+TRANSCRIPT_BOX_ID);
  if (box) { const d = box.elt; d.scrollTop = d.scrollHeight; }
}

// optional: status bar helper
function msg(s){ if (statusDiv) statusDiv.html(s); }

// Space = PTT
function keyPressed(){ if (keyCode === 32) startPTT(); }
function keyReleased(){ if (keyCode === 32) stopPTT(); }
